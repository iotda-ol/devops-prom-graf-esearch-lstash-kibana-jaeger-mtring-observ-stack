# Logstash Pipeline Configuration
# Production-ready log processing pipeline  

# ==============================================================================
# INPUT PLUGINS - Receive logs from various sources
# ==============================================================================

input {
  # Beats input - Receive logs from Filebeat, Metricbeat, etc.
  beats {
    port => 5044
    codec => json
    tags => ["beats"]
  }

  # Syslog input - Receive syslog messages
  syslog {
    port => 5000
    type => "syslog"
    tags => ["syslog"]
  }

  # HTTP input - Receive logs via HTTP POST
  http {
    port => 8080
    codec => json
    tags => ["http"]
  }

  # TCP input - Receive raw TCP logs
  tcp {
    port => 5001
    codec => json_lines
    tags => ["tcp"]
  }
}

# ==============================================================================
# FILTER PLUGINS - Parse, enrich, and transform logs
# ==============================================================================

filter {
  # -----------------------------------------------------------------------------
  # JSON Parsing - Parse JSON formatted logs
  # -----------------------------------------------------------------------------
  if [type] == "app" or "http" in [tags] {
    json {
      source => "message"
      target => "parsed"
      skip_on_invalid_json => true
    }

    # Extract common fields from parsed JSON
    if [parsed] {
      mutate {
        rename => {
          "[parsed][level]" => "log_level"
          "[parsed][timestamp]" => "log_timestamp"
          "[parsed][logger]" => "logger_name"
          "[parsed][message]" => "log_message"
        }
        remove_field => ["parsed"]
      }
    }
  }

  # -----------------------------------------------------------------------------
  # Grok Parsing - Extract fields using patterns
  # -----------------------------------------------------------------------------
  
  # Parse Apache/Nginx access logs
  if [type] == "nginx" or [type] == "apache" {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
      add_tag => ["web_access"]
    }
  }

  # Parse application logs with custom pattern
  if [type] == "application" {
    grok {
      match => { "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] \[%{LOGLEVEL:level}\] \[%{DATA:logger}\] - %{GREEDYDATA:msg}" }
    }
  }

  # Extract HTTP status code categories
  if [response] {
    if [response] >= 500 {
      mutate { add_tag => ["error_5xx"] }
    } else if [response] >= 400 {
      mutate { add_tag => ["error_4xx"] }
    } else if [response] >= 300 {
      mutate { add_tag => ["redirect_3xx"] }
    } else if [response] >= 200 {
      mutate { add_tag => ["success_2xx"] }
    }
  }

  # -----------------------------------------------------------------------------
  # Kubernetes Log Processing
  # -----------------------------------------------------------------------------
  if [kubernetes] {
    # Extract Kubernetes metadata
    mutate {
      add_field => {
        "k8s_namespace" => "%{[kubernetes][namespace]}"
        "k8s_pod" => "%{[kubernetes][pod][name]}"
        "k8s_container" => "%{[kubernetes][container][name]}"
        "k8s_node" => "%{[kubernetes][node][name]}"
      }
    }

    # Parse container logs that are JSON
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
        target => "container_log"
      }
    }
  }

  # -----------------------------------------------------------------------------
  # Date Parsing - Normalize timestamps
  # -----------------------------------------------------------------------------
  if [log_timestamp] {
    date {
      match => [ "log_timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss,SSS", "yyyy-MM-dd HH:mm:ss" ]
      target => "@timestamp"
    }
  }

  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss,SSS" ]
      target => "@timestamp"
    }
  }

  # -----------------------------------------------------------------------------
  # GeoIP Enrichment - Add geographic data for IP addresses
  # -----------------------------------------------------------------------------
  if [clientip] or [client_ip] or [remote_addr] {
    geoip {
      source => "[clientip]"
      target => "geoip"
      database => "/usr/share/logstash/geoip/GeoLite2-City.mmdb"
    }

    # Also try alternative field names
    if ![geoip] {
      geoip {
        source => "[client_ip]"
        target => "geoip"
      }
    }

    if ![geoip] {
      geoip {
        source => "[remote_addr]"
        target => "geoip"
      }
    }
  }

  # -----------------------------------------------------------------------------
  # User Agent Parsing - Extract browser and device info
  # -----------------------------------------------------------------------------
  if [agent] or [user_agent] or [useragent] {
    useragent {
      source => "agent"
      target => "user_agent_info"
    }

    if ![user_agent_info] {
      useragent {
        source => "user_agent"
        target => "user_agent_info"
      }
    }
  }

  # -----------------------------------------------------------------------------
  # Field Mutations - Clean up and standardize
  # -----------------------------------------------------------------------------
  
  # Remove unnecessary fields
  mutate {
    remove_field => ["@version", "host"]
  }

  # Add environment tag if not present
  if ![environment] {
    mutate {
      add_field => { "environment" => "unknown" }
    }
  }

  # Normalize log levels to lowercase
  if [log_level] {
    mutate {
      lowercase => ["log_level"]
    }
  }

  # Convert numeric fields
  if [response_time] {
    mutate {
      convert => { "response_time" => "float" }
    }
  }

  if [bytes] {
    mutate {
      convert => { "bytes" => "integer" }
    }
  }

  # -----------------------------------------------------------------------------
  # Error Detection - Tag error logs
  # -----------------------------------------------------------------------------
  if [log_level] == "error" or [log_level] == "fatal" or [log_level] == "critical" {
    mutate {
      add_tag => ["error"]
    }
  }

  # Tag logs containing exception stack traces
  if [message] =~ /Exception|Error|Traceback/ {
    mutate {
      add_tag => ["exception"]
    }
  }
}

# ==============================================================================
# OUTPUT PLUGINS - Send processed logs to destinations
# ==============================================================================

output {
  # Send all logs to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    
    # Index naming pattern - daily indices
    index => "logs-%{+YYYY.MM.dd}"
    
    # Document ID (optional) - prevents duplicates
    # document_id => "%{[@metadata][fingerprint]}"
    
    # Template management
    manage_template => true
    template_name => "logs"
    template_overwrite => false
  }

  # Debug output - uncomment for troubleshooting
  # stdout {
  #   codec => rubydebug {
  #     metadata => true
  #   }
  # }

  # Conditional outputs based on tags
  
  # Send errors to separate index
  if "error" in [tags] or "exception" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "logs-errors-%{+YYYY.MM.dd}"
    }
  }

  # Send Kubernetes logs to dedicated index
  if [kubernetes] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "logs-kubernetes-%{+YYYY.MM.dd}"
    }
  }

  # Send metrics to Prometheus (if pushgateway is available)
  # if "metrics" in [tags] {
  #   http {
  #     url => "http://pushgateway:9091/metrics/job/logstash"
  #     http_method => "post"
  #     format => "message"
  #     content_type => "text/plain"
  #   }
  # }
}
